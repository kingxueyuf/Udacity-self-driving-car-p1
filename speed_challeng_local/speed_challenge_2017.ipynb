{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alex output shape :  torch.Size([20, 5, 68096])\n",
      "lstm input shape :  torch.Size([20, 5, 68096])\n",
      "lstm output shape :  torch.Size([20, 5, 420])\n",
      "fc input shape :  torch.Size([20, 5, 420])\n",
      "fc output shape :  torch.Size([20, 5, 1])\n",
      "predict shape :  torch.Size([5, 20])\n",
      "label shape :  torch.Size([5, 20])\n",
      "------ PREDICT start------\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0528 -0.0501 -0.0402 -0.0523 -0.0621 -0.0595 -0.0522 -0.0737 -0.0703 -0.0833\n",
      "-0.0471 -0.0651 -0.0431 -0.0461 -0.0311 -0.0459 -0.0753 -0.0618 -0.0606 -0.0591\n",
      "-0.0712 -0.0484 -0.0675 -0.0762 -0.1137 -0.1128 -0.1082 -0.0889 -0.0826 -0.0643\n",
      "-0.0564 -0.0493 -0.0523 -0.0535 -0.0248 -0.0460 -0.0164 -0.0562 -0.0058 -0.0080\n",
      "-0.0124 -0.0393 -0.0627 -0.0125 -0.0547 -0.0464 -0.0008  0.0056 -0.0288  0.0058\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.0611 -0.1059 -0.0296 -0.0478 -0.0294 -0.0522 -0.0313 -0.0440 -0.0868 -0.0727\n",
      "-0.0761 -0.0386 -0.0445 -0.0430 -0.0440 -0.0684 -0.0480 -0.0490 -0.0517 -0.0413\n",
      "-0.0493 -0.0591 -0.0417 -0.0398 -0.0327 -0.0565 -0.0492 -0.0566 -0.0476 -0.0583\n",
      "-0.0317  0.0135  0.0052  0.0144 -0.0335 -0.0157  0.0121  0.0069 -0.0086 -0.0323\n",
      "-0.0011 -0.0230 -0.0320  0.0121  0.0092 -0.0230 -0.0261 -0.0169 -0.0199 -0.0384\n",
      "[torch.FloatTensor of size 5x20]\n",
      "\n",
      "------ PREDICT   end------\n",
      "------ LABEL start------\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 7 \n",
      " 23.7977  23.8030  23.7634  23.7408  23.7485  23.7302  23.7225  23.7596\n",
      " 22.1017  22.0899  22.0946  22.1009  22.1331  22.1295  22.1285  22.1316\n",
      " 23.1955  23.1986  23.1778  23.1909  23.1582  23.1545  23.1550  23.1566\n",
      " 10.2629  10.2523  10.2366  10.2256  10.2060  10.2517  10.2481  10.2546\n",
      "  0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000\n",
      "\n",
      "Columns 8 to 15 \n",
      " 23.7733  23.7687  23.7962  23.7112  23.7490  23.7786  23.7066  23.7289\n",
      " 22.1397  22.1694  22.1637  22.1714  22.1532  22.1598  22.1616  22.1647\n",
      " 23.1535  23.1393  23.1316  23.1098  23.1124  23.1186  23.0947  23.1041\n",
      " 10.2446  10.2236  10.2407  10.2160  10.2733  10.2622  10.2390  10.2707\n",
      "  0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000\n",
      "\n",
      "Columns 16 to 19 \n",
      " 23.7767  23.7398  23.8036  23.7915\n",
      " 22.1452  22.1567  22.1887  22.1860\n",
      " 23.0928  23.0831  23.0794  23.0975\n",
      " 10.2346  10.3163  10.3035  10.3088\n",
      "  0.0000   0.0000   0.0000   0.0000\n",
      "[torch.FloatTensor of size 5x20]\n",
      "\n",
      "------ LABEL   end------\n",
      "loss shape :  torch.Size([1])\n",
      "[epoch : 1, iteration :     1] loss: 17.038\n",
      "alex output shape :  torch.Size([20, 5, 68096])\n",
      "lstm input shape :  torch.Size([20, 5, 68096])\n",
      "lstm output shape :  torch.Size([20, 5, 420])\n",
      "fc input shape :  torch.Size([20, 5, 420])\n",
      "fc output shape :  torch.Size([20, 5, 1])\n",
      "predict shape :  torch.Size([5, 20])\n",
      "label shape :  torch.Size([5, 20])\n",
      "------ PREDICT start------\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "1.00000e-02 *\n",
      " -2.9419 -1.1506  0.4786 -3.2445 -2.5512 -1.9053 -2.2838 -2.9580  1.2749  2.9013\n",
      " -2.3398  0.5294 -0.8830  0.9502 -2.2809  0.9397  1.1955  1.1065 -0.2597 -0.8233\n",
      " -1.7397 -0.3231 -1.8443  0.9279  1.5639  0.5024  0.0568 -1.2840  1.3397  1.0318\n",
      " -2.7001 -1.7140 -2.6222 -1.4101 -1.0902  1.6425 -0.4025 -1.9546 -1.6979 -4.8396\n",
      " -2.3948 -2.8800 -1.7671 -0.4877 -1.9008 -2.4158 -1.2932  0.3418 -0.6612  0.3314\n",
      "\n",
      "Columns 10 to 19 \n",
      "1.00000e-02 *\n",
      "  2.3140  1.3816  1.4546 -2.8071  0.8286  1.1481  0.0914  0.2749 -0.4845 -1.8232\n",
      "  0.7370 -1.0479  0.4548 -2.3986 -1.1396  1.6432  0.5309  2.1774 -2.9574  2.7147\n",
      "  1.1378 -1.1237  4.2003  1.3516  2.8667  2.1824  1.3941 -1.3314 -3.3127  0.4655\n",
      " -3.8829 -2.4132 -2.1215 -5.0300 -2.3122 -3.1867 -4.5833 -6.0297 -4.0965 -2.1551\n",
      " -2.5041 -0.2861  0.8089  2.2310 -1.5101 -2.3358 -1.2323 -3.7746  2.0809  1.6547\n",
      "[torch.FloatTensor of size 5x20]\n",
      "\n",
      "------ PREDICT   end------\n",
      "------ LABEL start------\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 7 \n",
      "  0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000\n",
      " 22.5021  22.5005  22.4498  22.4556  22.4346  22.4405  22.4240  22.4349\n",
      " 15.4385  15.3760  15.3690  15.3670  15.3092  15.2903  15.2258  15.2357\n",
      "  7.5188   7.5210   7.5559   7.6107   7.6242   7.6311   7.6669   7.6642\n",
      " 23.3975  23.3966  23.4136  23.4375  23.4628  23.4868  23.5042  23.5002\n",
      "\n",
      "Columns 8 to 15 \n",
      "  0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000\n",
      " 22.4350  22.4165  22.4209  22.4150  22.3853  22.3983  22.4321  22.4217\n",
      " 15.1932  14.9519  15.0435  15.0575  15.1418  15.2234  15.0948  14.9416\n",
      "  7.6747   7.6616   7.6578   7.6733   7.6892   7.6877   7.6695   7.6700\n",
      " 23.5174  23.5237  23.5285  23.5780  23.5857  23.5624  23.6193  23.6138\n",
      "\n",
      "Columns 16 to 19 \n",
      "  0.0000   0.0000   0.0000   0.0000\n",
      " 22.4289  22.3878  22.3735  22.3829\n",
      " 14.8621  15.0404  14.8629  14.9014\n",
      "  7.6318   7.5851   7.4815   7.5116\n",
      " 23.6144  23.6359  23.6621  23.6473\n",
      "[torch.FloatTensor of size 5x20]\n",
      "\n",
      "------ LABEL   end------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3068178f26eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------ LABEL   end------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable as V\n",
    "import torch as th\n",
    "from torchvision import models\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "\n",
    "class AlexLSTM(nn.Module):\n",
    "    def __init__(self, n_layers=2, h_size=420):\n",
    "        super(AlexLSTM, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        alexnet = models.alexnet(pretrained=True)\n",
    "        self.conv = nn.Sequential(*list(alexnet.children())[:-1])\n",
    "\n",
    "        self.lstm = nn.LSTM(68096, h_size, dropout=0.2, num_layers=n_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(h_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, 3, time_stamp, 480, 640)\n",
    "        batch_size, timesteps = x.size()[0], x.size()[2]\n",
    "        state = self._init_state(b_size=batch_size)\n",
    "\n",
    "        convs = []\n",
    "        for t in range(timesteps):\n",
    "            conv = self.conv(x[:, :, t, :, :])\n",
    "#             print(\"conv shape : \", conv.size())\n",
    "            conv = conv.view(batch_size, -1)\n",
    "#             print(\"conv reshape :\", conv.size())\n",
    "            convs.append(conv)\n",
    "        convs = th.stack(convs, 0)\n",
    "        print(\"alex output shape : \",convs.size()) # ([20, 5, 68096]) (seq_len, batch, input_size)\n",
    "        print(\"lstm input shape : \",convs.size())\n",
    "        lstm, _ = self.lstm(convs, state) # lstm input (seq_len, batch, input_size)\n",
    "        print(\"lstm output shape : \",lstm.size()) # torch.Size([20, 5, 420]) (seq_len, batch, hidden_size * num_directions)\n",
    "        print(\"fc input shape : \",lstm.size())\n",
    "        logit = self.fc(lstm) # seq_len, batch, input_size ([20, 5, 1])\n",
    "        print(\"fc output shape : \",logit.size())\n",
    "        \n",
    "        logit = logit.transpose(1,0).squeeze(2)\n",
    "        return logit\n",
    "\n",
    "    def _init_state(self, b_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (\n",
    "            V(weight.new(self.n_layers, b_size, self.h_size).normal_(0.0, 0.01)),\n",
    "            V(weight.new(self.n_layers, b_size, self.h_size).normal_(0.0, 0.01))\n",
    "        )\n",
    "    \n",
    "net = AlexLSTM()\n",
    "# print(net)\n",
    "\n",
    "batch_size = 5\n",
    "time_stamp = 20\n",
    "train_dataset = os.listdir(\"img/\")\n",
    "total_img_num = len(train_dataset)\n",
    "iteration_per_epoch = int(total_img_num / (batch_size*time_stamp))\n",
    "lr = 0.0001\n",
    "criterion = nn.MSELoss(False)\n",
    "\n",
    "def fetch_image_and_label(batch_size, time_stamp):\n",
    "    numbers = []\n",
    "    while(len(numbers) != batch_size):\n",
    "        a = random.randint(0,total_img_num-time_stamp)\n",
    "        if a not in numbers:\n",
    "            numbers.append(a)\n",
    "    label = []\n",
    "    file_in = open('data/train.txt', 'r')\n",
    "    for line in file_in.readlines():\n",
    "        label.append(float(line))\n",
    "    \n",
    "    x = np.zeros((batch_size, time_stamp, 480, 640, 3))\n",
    "    y = np.zeros((batch_size, time_stamp))\n",
    "    for i in range(batch_size):\n",
    "        for j in range(time_stamp):\n",
    "            img_name = numbers[i] + j\n",
    "            image_path = 'img/frame' + str(img_name) + \".jpg\"\n",
    "            img = cv2.imread(image_path)\n",
    "            x[i,j] = img\n",
    "            y[i,j] = label[numbers[i] + j]\n",
    "    \n",
    "    x = x.transpose(0, 4, 1, 2, 3) # (batch_size, 3, time_stamp, 480, 640)\n",
    "    return x, y\n",
    "    \n",
    "min_loss = 100\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i in range(iteration_per_epoch):\n",
    "        x,y = fetch_image_and_label(batch_size, time_stamp)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        x = V(th.from_numpy(x).float())\n",
    "        y = V(th.from_numpy(y).float())\n",
    "\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "        optimizer.zero_grad()# zero the parameter gradients\n",
    "        # forward + backward + optimize\n",
    "        predict = net(x)\n",
    "        \n",
    "        print(\"predict shape : \", predict.size())\n",
    "        print(\"label shape : \", y.size())\n",
    "        print(\"------ PREDICT start------\")\n",
    "        print(predict)\n",
    "        print(\"------ PREDICT   end------\")\n",
    "        print(\"------ LABEL start------\")\n",
    "        print(y)\n",
    "        print(\"------ LABEL   end------\")\n",
    "        loss = criterion(predict, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        print(\"loss shape : \",loss.data.size())\n",
    "        running_loss += loss.data[0]\n",
    "        if running_loss <= min_loss :\n",
    "            min_loss = running_loss\n",
    "            print(\"Saving model ...\")\n",
    "            th.save(net.state_dict(), 'save/%d_%s.p' % (i, epoch))\n",
    "        print('[epoch : %d, iteration : %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "        running_loss = 0.0\n",
    "    print(\"Saving model ...\")\n",
    "    th.save(net.state_dict(), 'save/epoch_%s.p' % (epoch))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
